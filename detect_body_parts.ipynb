{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect Body Parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before to run this notebook run first dataset_framing notebook\n",
    "\n",
    "To see a more detailed execution with graphical examples take a look to the openpose_example notebook\n",
    "\n",
    "What this notebook does:\n",
    "\n",
    "for each image:\n",
    "\n",
    "- takes in input an image from the framed_dataset\n",
    "- for each person in the image, detect body parts\n",
    "- for each person in the image, build a skeleton\n",
    "- save the result in a json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import cv2\n",
    "import matplotlib\n",
    "import pylab as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import tensorflow as tf\n",
    "\n",
    "from models.config import get_default_configuration\n",
    "from tensorflow.keras.models import load_model\n",
    "from models.model import get_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the keras model load the weights file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_path = \"models/weights.h5\"\n",
    "model = get_model(1.0, 224)\n",
    "model.load_weights(weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "numFrame=0\n",
    "#for numFrame...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id Frame: 0\n",
      "Person: 1\n",
      "nose ( 160.0 - 88.0 )\n",
      "neck ( 160.0 - 104.0 )\n",
      "right_shoulder ( 152.0 - 104.0 )\n",
      "right_elbow ( 152.0 - 128.0 )\n",
      "left_shoulder ( 176.0 - 104.0 )\n",
      "left_elbow ( 176.0 - 128.0 )\n",
      "left_wrist ( 160.0 - 104.0 )\n",
      "right_hip ( 160.0 - 136.0 )\n",
      "left_hip ( 168.0 - 152.0 )\n",
      "left_knee ( 160.0 - 176.0 )\n",
      "left_ankle ( 160.0 - 184.0 )\n",
      "right_eye ( 152.0 - 80.0 )\n",
      "left_eye ( 160.0 - 80.0 )\n",
      "left_ear ( 168.0 - 88.0 )\n",
      "Person: 2\n",
      "nose ( 104.0 - 88.0 )\n",
      "neck ( 112.0 - 104.0 )\n",
      "right_shoulder ( 104.0 - 104.0 )\n",
      "right_elbow ( 104.0 - 128.0 )\n",
      "right_wrist ( 96.0 - 136.0 )\n",
      "left_shoulder ( 120.0 - 112.0 )\n",
      "left_elbow ( 128.0 - 136.0 )\n",
      "left_wrist ( 104.0 - 144.0 )\n",
      "right_hip ( 112.0 - 152.0 )\n",
      "right_knee ( 112.0 - 184.0 )\n",
      "left_hip ( 120.0 - 152.0 )\n",
      "left_knee ( 120.0 - 176.0 )\n",
      "left_eye ( 104.0 - 88.0 )\n",
      "left_ear ( 112.0 - 88.0 )\n"
     ]
    }
   ],
   "source": [
    "path = \"dataset_framing/framed_dataset/Violence/img0.jpg\" #+ str(numFrame)\n",
    "#path = \"framed_dataset/NonViolence/img\" + str(i)\n",
    "oriImg = cv2.imread(path)\n",
    "input_img = oriImg[np.newaxis,...] \n",
    "output_blobs = model.predict(input_img) #generate heatmaps and paf vectors\n",
    "heatmap = output_blobs[3]\n",
    "heatmap = heatmap[0]\n",
    "paf = output_blobs[2] \n",
    "paf = paf[0]\n",
    "config = get_default_configuration()\n",
    "all_peaks = []\n",
    "peak_counter = 0\n",
    "thre1 = 0.1\n",
    "hashMap = dict() \n",
    "\n",
    "for part_meta in config.body_parts.values():\n",
    "    map = heatmap[:, :, part_meta.heatmap_idx]    \n",
    "    \n",
    "    map_left = np.zeros(map.shape)\n",
    "    map_left[1:,:] = map[:-1,:]\n",
    "    map_right = np.zeros(map.shape)\n",
    "    map_right[:-1,:] = map[1:,:]\n",
    "    map_up = np.zeros(map.shape)\n",
    "    map_up[:,1:] = map[:,:-1]\n",
    "    map_down = np.zeros(map.shape)\n",
    "    map_down[:,:-1] = map[:,1:]\n",
    "    \n",
    "    peaks_binary = np.logical_and.reduce((map>=map_left, map>=map_right, map>=map_up, map>=map_down, map > thre1))\n",
    "    peaks = list(zip(np.nonzero(peaks_binary)[1], np.nonzero(peaks_binary)[0])) # note reverse\n",
    "    peaks_with_score = [x + (map[x[1],x[0]],) for x in peaks]\n",
    "    id = range(peak_counter, peak_counter + len(peaks))\n",
    "    peaks_with_score_and_id = [peaks_with_score[i] + (id[i],) for i in range(len(id))]\n",
    "\n",
    "    for i in id:\n",
    "        hashMap[i]=part_meta.body_part.name \n",
    "        \n",
    "    all_peaks.append(peaks_with_score_and_id)\n",
    "    peak_counter += len(peaks)\n",
    "# find connection in the specified sequence, center 29 is in the position 15\n",
    "limbSeq = [[2,3], [2,6], [3,4], [4,5], [6,7], [7,8], [2,9], [9,10], \\\n",
    "           [10,11], [2,12], [12,13], [13,14], [2,1], [1,15], [15,17], \\\n",
    "           [1,16], [16,18], [3,17], [6,18]]\n",
    "# the middle joints heatmap correpondence\n",
    "mapIdx = [[31,32], [39,40], [33,34], [35,36], [41,42], [43,44], [19,20], [21,22], \\\n",
    "          [23,24], [25,26], [27,28], [29,30], [47,48], [49,50], [53,54], [51,52], \\\n",
    "          [55,56], [37,38], [45,46]]\n",
    "thre2 = 0.05\n",
    "\n",
    "connection_all = []\n",
    "special_k = []\n",
    "mid_num = 10\n",
    "\n",
    "for k in range(len(mapIdx)):\n",
    "    score_mid = paf[:,:,[x-19 for x in mapIdx[k]]]\n",
    "    candA = all_peaks[limbSeq[k][0]-1]\n",
    "    candB = all_peaks[limbSeq[k][1]-1]\n",
    "    nA = len(candA)\n",
    "    nB = len(candB)\n",
    "    indexA, indexB = limbSeq[k]\n",
    "    if(nA != 0 and nB != 0):\n",
    "        connection_candidate = []\n",
    "        for i in range(nA):\n",
    "            for j in range(nB):\n",
    "                vec = np.subtract(candB[j][:2], candA[i][:2])\n",
    "                norm = math.sqrt(vec[0]*vec[0] + vec[1]*vec[1])\n",
    "                # failure case when 2 body parts overlaps\n",
    "                if norm == 0:\n",
    "                    continue\n",
    "                vec = np.divide(vec, norm)\n",
    "                \n",
    "                startend = list(zip(np.linspace(candA[i][0], candB[j][0], num=mid_num), \\\n",
    "                               np.linspace(candA[i][1], candB[j][1], num=mid_num)))\n",
    "                \n",
    "                vec_x = np.array([score_mid[int(round(startend[I][1])), int(round(startend[I][0])), 0] \\\n",
    "                                  for I in range(len(startend))])\n",
    "                vec_y = np.array([score_mid[int(round(startend[I][1])), int(round(startend[I][0])), 1] \\\n",
    "                                  for I in range(len(startend))])\n",
    "\n",
    "                score_midpts = np.multiply(vec_x, vec[0]) + np.multiply(vec_y, vec[1])\n",
    "                score_with_dist_prior = sum(score_midpts)/len(score_midpts) + min(0.5*oriImg.shape[0]/norm-1, 0)\n",
    "                criterion1 = len(np.nonzero(score_midpts > thre2)[0]) > 0.8 * len(score_midpts)\n",
    "                criterion2 = score_with_dist_prior > 0\n",
    "                if criterion1 and criterion2:\n",
    "                    connection_candidate.append([i, j, score_with_dist_prior, score_with_dist_prior+candA[i][2]+candB[j][2]])\n",
    "\n",
    "        connection_candidate = sorted(connection_candidate, key=lambda x: x[2], reverse=True)\n",
    "        connection = np.zeros((0,5))\n",
    "        for c in range(len(connection_candidate)):\n",
    "            i,j,s = connection_candidate[c][0:3]\n",
    "            if(i not in connection[:,3] and j not in connection[:,4]):\n",
    "                connection = np.vstack([connection, [candA[i][3], candB[j][3], s, i, j]])\n",
    "                if(len(connection) >= min(nA, nB)):\n",
    "                    break\n",
    "\n",
    "        connection_all.append(connection)\n",
    "    else:\n",
    "        special_k.append(k)\n",
    "        connection_all.append([])\n",
    "subset = -1 * np.ones((0, 20))\n",
    "candidate = np.array([item for sublist in all_peaks for item in sublist])\n",
    "\n",
    "for k in range(len(mapIdx)):\n",
    "    if k not in special_k:\n",
    "        partAs = connection_all[k][:,0]\n",
    "        partBs = connection_all[k][:,1]\n",
    "        indexA, indexB = np.array(limbSeq[k]) - 1\n",
    "\n",
    "        for i in range(len(connection_all[k])): \n",
    "            found = 0\n",
    "            subset_idx = [-1, -1]\n",
    "            for j in range(len(subset)): \n",
    "                if subset[j][indexA] == partAs[i] or subset[j][indexB] == partBs[i]:\n",
    "                    subset_idx[found] = j\n",
    "                    found += 1\n",
    "            \n",
    "            if found == 1:\n",
    "                j = subset_idx[0]\n",
    "                if(subset[j][indexB] != partBs[i]):\n",
    "                    subset[j][indexB] = partBs[i]\n",
    "                    subset[j][-1] += 1\n",
    "                    subset[j][-2] += candidate[partBs[i].astype(int), 2] + connection_all[k][i][2]\n",
    "            elif found == 2: # if found 2 and disjoint, merge them\n",
    "                j1, j2 = subset_idx\n",
    "                membership = ((subset[j1]>=0).astype(int) + (subset[j2]>=0).astype(int))[:-2]\n",
    "                if len(np.nonzero(membership == 2)[0]) == 0: #merge\n",
    "                    subset[j1][:-2] += (subset[j2][:-2] + 1)\n",
    "                    subset[j1][-2:] += subset[j2][-2:]\n",
    "                    subset[j1][-2] += connection_all[k][i][2]\n",
    "                    subset = np.delete(subset, j2, 0)\n",
    "                else: # as like found == 1\n",
    "                    subset[j1][indexB] = partBs[i]\n",
    "                    subset[j1][-1] += 1\n",
    "                    subset[j1][-2] += candidate[partBs[i].astype(int), 2] + connection_all[k][i][2]\n",
    "\n",
    "            # if find no partA in the subset, create a new subset\n",
    "            elif not found and k < 17:\n",
    "                row = -1 * np.ones(20)\n",
    "                row[indexA] = partAs[i]\n",
    "                row[indexB] = partBs[i]\n",
    "                row[-1] = 2\n",
    "                row[-2] = sum(candidate[connection_all[k][i,:2].astype(int), 2]) + connection_all[k][i][2]\n",
    "                subset = np.vstack([subset, row])\n",
    "                \n",
    "# delete some rows of subset which has few parts occur\n",
    "deleteIdx = [];\n",
    "for i in range(len(subset)):\n",
    "    if subset[i][-1] < 4 or subset[i][-2]/subset[i][-1] < 0.4:\n",
    "        deleteIdx.append(i)\n",
    "subset = np.delete(subset, deleteIdx, axis=0)\n",
    "scale = 8\n",
    "\n",
    "#questo print deve essere fatto nel file json, non qui\n",
    "print(\"id Frame: \" + str(numFrame))\n",
    "for i in range(len(subset)):\n",
    "    print(\"Person: \" + str(i+1))\n",
    "    for j in range(len(subset[i])-2):\n",
    "        cella = subset[i][j]\n",
    "        if cella != -1:\n",
    "            X = candidate[cella.astype(int), 0] * scale\n",
    "            Y = candidate[cella.astype(int), 1] * scale\n",
    "            print(hashMap[cella],\"(\",X,\"-\",Y,\")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
